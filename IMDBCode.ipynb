{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b12b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torchtext as tt\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ADD_DATA = True #whether to use augmented data\n",
    "FORCE_BALANCE = False # whether to force balanced labels \n",
    "\n",
    "select_len = 300 #select only samples shorter than this\n",
    "cut_off_len = 300 #cut off all samples to this length so they fit into memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554063ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "1.5734536082474226\n",
      "1.5\n",
      "776\n",
      "2391\n",
      "1.356754496026767\n"
     ]
    }
   ],
   "source": [
    "#take IMDB dataset (train and test), split into data and label, and adjust samples and labels according to parameters\n",
    "\n",
    "ds_train, ds_test = tt.datasets.IMDB()\n",
    "\n",
    "def separate(ds):\n",
    "    ds_text = []\n",
    "    ds_label = []\n",
    "    for label,text in iter(ds):\n",
    "        ds_text.append(text)\n",
    "        ds_label.append(label)\n",
    "    return ds_text,ds_label\n",
    "\n",
    "ds_train_text, ds_train_label = separate(ds_train)\n",
    "\n",
    "ds_test_text, ds_test_label = separate(ds_test)\n",
    "\n",
    "print('full dataset average label',np.mean(ds_train_label))\n",
    "\n",
    "#reduce dataset to short samples\n",
    "\n",
    "#use the commented code to just select 2500 indices from the full dataset\n",
    "#np.random.choice(range(len(ds_train_text)),size = 2500)\n",
    "indices = [i for i in range(len(ds_train_text)) if len(ds_train_text[i]) < select_len] \n",
    "\n",
    "ds_train_text = [ds_train_text[i] for i in indices]\n",
    "ds_train_label = [ds_train_label[i] for i in indices]\n",
    "\n",
    "print('reduced dataset average label',np.mean(ds_train_label))\n",
    "\n",
    "print('test dataset average label',np.mean(ds_test_label))\n",
    "\n",
    "print('reduced dataset size',len(ds_train_text))\n",
    "\n",
    "#add agumented data\n",
    "if ADD_DATA:\n",
    "    for line in open('./clean_data.out'):\n",
    "        m = re.match(r'(1|2)\\t(.*)',line) # match label<tab>text format\n",
    "        if m:\n",
    "            ds_train_text.append(m.group(2))\n",
    "            ds_train_label.append(int(m.group(1)))\n",
    "            \n",
    "#enforce 50/50 balanced label\n",
    "if FORCE_BALANCE:\n",
    "    label_counts = [(ds_train_label.count(label),label) for label in np.unique(ds_train_label)]\n",
    "    to_add_label_count, to_add_label = min(label_counts)\n",
    "    target_count, _ = max(label_counts)\n",
    "    num_to_add = target_count - to_add_label_count\n",
    "    if num_to_add > 0:\n",
    "        to_add_indices = [i for i in range(len(ds_train_text)) if ds_train_label[i] == to_add_label]\n",
    "        chosen_indices = np.random.choice(to_add_indices,size = num_to_add)\n",
    "        for i in chosen_indices:\n",
    "            ds_train_text.append(ds_train_text[i])\n",
    "            ds_train_label.append(ds_train_label[i])\n",
    "        \n",
    "        \n",
    "    \n",
    "print('final dataset size',len(ds_train_text))\n",
    "print('final dataset average label',np.mean(ds_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5331161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#just disassemble the sentences into tokens\n",
    "def yield_tokens(data_iter,tokenizer): \n",
    "    for text in data_iter: \n",
    "        yield tokenizer(text) \n",
    "\n",
    "#create vocab from data\n",
    "def get_vocab(train_data,tokenizer): \n",
    "    vocab = tt.vocab.build_vocab_from_iterator(yield_tokens(train_data,tokenizer), \n",
    "                                     specials=['<UNK>', '<PAD>','<SOS>','<EOS>'], \n",
    "                                     max_tokens=50002) \n",
    "    vocab.set_default_index(vocab['<UNK>']) \n",
    "    return vocab \n",
    "\n",
    "#receives text and returns torch tensor and vocab.\n",
    "#torch tensor is truncated text translated to token numbers according to vocab.\n",
    "#if vocab is None then a vocabulary will be created based on the text\n",
    "def process_dataset(text,vocab = None):\n",
    "    \n",
    "    tokenizer = tt.data.utils.get_tokenizer('basic_english')\n",
    "    \n",
    "    tokens = [['<SOS>']+tokenizer(s)+['<EOS>'] for s in text]\n",
    "    \n",
    "    if vocab == None:\n",
    "        vocab = get_vocab(text,tokenizer)\n",
    "    \n",
    "    max_len = max([len(l) for l in tokens])\n",
    "    \n",
    "    pad = lambda x: x + ['<PAD>']*(max_len - len(x))\n",
    "    \n",
    "    data = torch.tensor([vocab.lookup_indices(pad(token_list)) for token_list in tokens],dtype=torch.int64)\n",
    "    \n",
    "    data = data[:,:cut_off_len]\n",
    "    \n",
    "    return data, vocab\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96285783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds_train_text,vocab = process_dataset(ds_train_text)\n",
    "ds_train_label = torch.tensor(ds_train_label) - 1 #change labels to 0 and 1 for easier handling\n",
    "\n",
    "ds_test_text,_ = process_dataset(ds_test_text,vocab)\n",
    "ds_test_label = torch.tensor(ds_test_label) - 1\n",
    "\n",
    "print(len(ds_train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a31de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embeddings(\n",
       "      (word_embeddings): Embedding(6932, 32, padding_idx=1)\n",
       "      (position_embeddings): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
       "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // self.num_heads  # here d is divided between the heads\n",
    "        # each head has hidden dimension d\n",
    "        \n",
    "        # These are still of dimension d_model. They will be split into number of heads \n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        batch_size = Q.size(0) \n",
    "        k_length = K.size(-2) \n",
    "        \n",
    "        # Scaling by d_k so that the soft(arg)max doesn't saturate\n",
    "        Q = Q / np.sqrt(self.d_k)                         # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(Q, K.transpose(2,3))          # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        A = torch.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\n",
    "        A = self.dropout(A)\n",
    "        \n",
    "        # Get the weighted average of the values\n",
    "        H = torch.matmul(A, V)     # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "        return H, A \n",
    "\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (heads X depth)\n",
    "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size, seq_length, dim = X_q.size() # dim = embedding dimension\n",
    "\n",
    "        # After transforming, split into num_heads \n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head)\n",
    "        \n",
    "        # Calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        H_cat = self.group_heads(H_cat, batch_size)    # (bs, q_length, dim)\n",
    "        \n",
    "        # Final linear layer  \n",
    "        H = self.W_h(H_cat)          # (bs, q_length, dim)\n",
    "        \n",
    "        return H, A\n",
    "    \n",
    "\"\"\"\n",
    "Feed Forward Network (FFN): an MLP with one hidden layer and ReLU activation applied to each and every element in the set.\n",
    "\"\"\"\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.k1convL1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.k1convL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.k1convL2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough `P`\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(0, max_len, dtype=torch.float32).reshape(-1, 1)\n",
    "        X = X / torch.pow(10_000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings, dropout=0):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        self.position_embeddings = PositionalEncoding(num_hiddens=d_model, dropout=self.dropout,\n",
    "                                                      max_len=max_position_embeddings)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Get position embeddings for the word embeddings and add them     \n",
    "        embeddings = self.position_embeddings(word_embeddings) # (bs, max_seq_length, dim)\n",
    "        \n",
    "        # Layer norm \n",
    "        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n",
    "        return embeddings\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.ffn = FFN(d_model, conv_hidden_dim)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Multi-head attention \n",
    "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Layer norm after adding the residual connection \n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Feed forward \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # Second layer norm after adding residual connection \n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding, dropout)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim, self.dropout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    \n",
    "# Transormer classifier for sentiment analysis\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size,\n",
    "                                          maximum_position_encoding=10000)\n",
    "        self.dense = nn.Linear(d_model, num_answers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2, \n",
    "                         conv_hidden_dim=128, input_vocab_size=len(vocab), num_answers=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678ecfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(ds_train_text, ds_train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=ds_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(ds_test_text, ds_test_label)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=ds_test, batch_size=batch_size)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "t_total = len(train_loader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f47116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    nb_batches = len(data_loader)\n",
    "    model.eval()\n",
    "    acc = 0 \n",
    "    for x,y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "                \n",
    "        out = model(x)\n",
    "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "\n",
    "    print(f\"eval accuracy: {acc / nb_batches}\")\n",
    "    \n",
    "    \n",
    "\n",
    "def train(train_loader):\n",
    "    for epoch in range(epochs):\n",
    "        nb_batches_train = len(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            out = model(x)\n",
    "\n",
    "            loss = f.cross_entropy(out, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            losses += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "                        \n",
    "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
    "        \n",
    "        print(f\"epoch {epoch}: train loss: {losses / nb_batches_train}\")\n",
    "        print(f\"training accuracy: {train_acc / nb_batches_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92997d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss: 0.6631056070327759\n",
      "training accuracy: 0.635270152752571\n",
      "epoch 1: train loss: 0.6531264876064501\n",
      "training accuracy: 0.6369385208711433\n",
      "epoch 2: train loss: 0.6437206268310547\n",
      "training accuracy: 0.6419199939503932\n",
      "epoch 3: train loss: 0.6301061856119257\n",
      "training accuracy: 0.6471944948578342\n",
      "epoch 4: train loss: 0.625851853897697\n",
      "training accuracy: 0.6450204174228675\n",
      "epoch 5: train loss: 0.6212571984843204\n",
      "training accuracy: 0.6562169162129462\n"
     ]
    }
   ],
   "source": [
    "train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7f9bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval accuracy: 0.5017139668367347\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfd27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2da3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
